{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eecedd1d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-14T08:10:06.116414Z",
     "iopub.status.busy": "2025-02-14T08:10:06.116145Z",
     "iopub.status.idle": "2025-02-14T08:10:07.069036Z",
     "shell.execute_reply": "2025-02-14T08:10:07.068047Z"
    },
    "papermill": {
     "duration": 0.957603,
     "end_time": "2025-02-14T08:10:07.070898",
     "exception": false,
     "start_time": "2025-02-14T08:10:06.113295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/university-students-complaints-and-reports/Datasetprojpowerbi.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd49f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:10:07.076896Z",
     "iopub.status.busy": "2025-02-14T08:10:07.076438Z",
     "iopub.status.idle": "2025-02-14T08:10:10.057944Z",
     "shell.execute_reply": "2025-02-14T08:10:10.057016Z"
    },
    "papermill": {
     "duration": 2.985327,
     "end_time": "2025-02-14T08:10:10.059204",
     "exception": false,
     "start_time": "2025-02-14T08:10:07.073877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "   15minut  30minut  abil  abl  abroad  academ  accent  accept  access  \\\n",
      "0        0        0     0    1       0       1       0       0       2   \n",
      "1        0        0     0    0       0       0       0       0       0   \n",
      "2        0        0     0    0       0       0       0       0       1   \n",
      "3        0        0     0    0       0       1       0       0       0   \n",
      "4        0        0     0    0       0       0       0       0       0   \n",
      "5        0        0     0    0       0       0       0       0       0   \n",
      "6        0        0     0    0       0       0       0       0       0   \n",
      "7        0        0     0    0       0       0       0       0       0   \n",
      "8        0        0     0    0       0       0       0       0       0   \n",
      "9        0        0     0    0       0       1       0       0       0   \n",
      "\n",
      "   accommod  ...  worth  would  write  writer  written  wrong  year  yet  \\\n",
      "0         0  ...      0      0      0       0        0      0     0    0   \n",
      "1         0  ...      0      0      0       0        0      0     0    0   \n",
      "2         0  ...      0      0      1       0        0      0     0    0   \n",
      "3         0  ...      0      0      0       0        0      0     0    0   \n",
      "4         0  ...      0      0      0       0        0      0     0    0   \n",
      "5         0  ...      0      0      0       0        0      0     0    0   \n",
      "6         0  ...      0      1      0       0        0      1     0    0   \n",
      "7         0  ...      0      0      0       0        0      0     0    0   \n",
      "8         0  ...      0      0      0       0        0      0     0    0   \n",
      "9         0  ...      0      0      0       0        0      0     0    0   \n",
      "\n",
      "   your  zone  \n",
      "0     0     0  \n",
      "1     0     0  \n",
      "2     0     0  \n",
      "3     0     0  \n",
      "4     0     0  \n",
      "5     0     0  \n",
      "6     0     0  \n",
      "7     0     0  \n",
      "8     0     0  \n",
      "9     0     0  \n",
      "\n",
      "[10 rows x 1311 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/university-students-complaints-and-reports/Datasetprojpowerbi.csv\")\n",
    "\n",
    "df[\"Reports_lower\"] = df[\"Reports\"].str.lower()\n",
    "\n",
    "translator = str.maketrans(\" \",\" \",string.punctuation)\n",
    "df[\"Reports_nopunct\"] = df[\"Reports_lower\"].str.translate(translator)\n",
    "\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "custom_stopwords = {\"im\",\"really\",\"student\",\"like\",\"feel\",\"ive\"}\n",
    "combined_stopwords = english_stopwords.union(custom_stopwords)\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    improved_text = [word for word in words if word not in combined_stopwords and word not in translator and not word.isdigit()]\n",
    "    return \" \".join(improved_text)\n",
    "\n",
    "df[\"cleaned_Reports\"] = df[\"Reports_nopunct\"].apply(remove_stopwords)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "df[\"stemmed_Reports\"] = df[\"cleaned_Reports\"].apply(stem_text)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"stemmed_Reports\"])\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "print(vectorized_df.head(10).tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1a83ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T08:10:10.063018Z",
     "iopub.status.busy": "2025-02-14T08:10:10.062737Z",
     "iopub.status.idle": "2025-02-14T08:10:10.071244Z",
     "shell.execute_reply": "2025-02-14T08:10:10.070057Z"
    },
    "papermill": {
     "duration": 0.012068,
     "end_time": "2025-02-14T08:10:10.072852",
     "exception": false,
     "start_time": "2025-02-14T08:10:10.060784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student      233\n",
      "univers      169\n",
      "make         138\n",
      "access       137\n",
      "academ       135\n",
      "limit        130\n",
      "find         130\n",
      "need         123\n",
      "time         121\n",
      "option       114\n",
      "opportun     113\n",
      "avail        113\n",
      "campu        113\n",
      "class        112\n",
      "hard         111\n",
      "difficult    107\n",
      "cours        104\n",
      "job           98\n",
      "onlin         96\n",
      "frustrat      96\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counting_words = vectorized_df.sum(axis=0)\n",
    "most_frequent_words = counting_words.sort_values(ascending=False)\n",
    "\n",
    "print(most_frequent_words.head(20))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3260867,
     "sourceId": 6264732,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.890405,
   "end_time": "2025-02-14T08:10:10.794244",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-14T08:10:03.903839",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
